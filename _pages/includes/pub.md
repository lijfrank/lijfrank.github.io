
# ðŸ“° Publication

â€  : Equal Contribution

<!-- <div class='paper-box'>
<div class='paper-box-image'><div class="badge">GraphCFC</div><img src='images/graphcfc.png' width="100%"></div>
<div class='paper-box-text' markdown="1">
<a href="https://doi.org/10.1109/TMM.2023.3260635" class="no-underline">GraphCFC: A directed graph based cross-modal feature complementation approach for multimodal conversational emotion recognition</a>  
**Jiang Li**, Xiaoping Wang, Guoqing Lv, Zhigang Zeng  
GraphCFC effectively extracts contextual and interactive information multimodal converesation. By employing multiple subspace extractors and a pair-wise cross-modal complementary (PairCC) strategy, GraphCFC alleviates the heterogeneity gap in multimodal fusion and extracts diverse information from multimodal dialogue graphs. GAT-MLP mitigates the over-smoothing issue in GNNs and offers a new structure for multimodal learning. By representing conversations as multimodal directed graphs and encoding various types of edges extracted from these graphs, the GAT-MLP layer is capable of precisely selecting crucial contextual and interactive information.
</div>
</div> -->

<!-- <div class='paper-box'>
<div class='paper-box-image'><div class="badge">TMM</div></div>
<div class='paper-box-text' markdown="1">
<a href="https://doi.org/10.1109/TMM.2023.3260635" class="no-underline">GraphCFC: A directed graph based cross-modal feature complementation approach for multimodal conversational emotion recognition</a>  
**Jiang Li**, Xiaoping Wang, Guoqing Lv, Zhigang Zeng  
IEEE Transactions on Multimedia (``TMM``), 2023  
[![](https://img.shields.io/badge/dynamic/json?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FLijfrank%2Flijfrank.github.io@google-scholar-stats%2Fgs_data.json&query=$['publications']['NesqTz8AAAAJ:as0KMg8qHbkC']['num_citations']&labelColor=f6f6f6&color=9cf&style=flat&label=Citations)](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=NesqTz8AAAAJ&citation_for_view=NesqTz8AAAAJ:as0KMg8qHbkC) [![](https://img.shields.io/github/languages/code-size/lijfrank/GraphCFC?style=social)](https://github.com/lijfrank/GraphCFC)
</div>
</div> -->

## Journal Paper

- <a href="https://doi.org/10.1109/TMM.2023.3260635" class="no-underline">GraphCFC: A directed graph based cross-modal feature complementation approach for multimodal conversational emotion recognition</a>  
**Jiang Li**, Xiaoping Wang, Guoqing Lv, Zhigang Zeng  
IEEE Transactions on Multimedia (``TMM``), 2023 [CAAI-A]  
[![](https://img.shields.io/badge/dynamic/json?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FLijfrank%2Flijfrank.github.io@google-scholar-stats%2Fgs_data.json&query=$['publications']['NesqTz8AAAAJ:as0KMg8qHbkC']['num_citations']&labelColor=f6f6f6&color=9cf&style=flat&label=Citations)](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=NesqTz8AAAAJ&citation_for_view=NesqTz8AAAAJ:as0KMg8qHbkC) [![](https://img.shields.io/github/languages/code-size/lijfrank/GraphCFC?style=social)](https://github.com/lijfrank/GraphCFC)

- <a href="https://doi.org/10.1109/TAFFC.2023.3261279" class="no-underline">GA2MIF: Graph and attention based two-stage multi-source information fusion for conversational emotion detection</a>  
**Jiang Li**, Xiaoping Wang, Guoqing Lv, Zhigang Zeng  
IEEE Transactions on Affective Computing (``TAFFC``), 2023 [CAAI-A]  
[![](https://img.shields.io/badge/dynamic/json?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FLijfrank%2Flijfrank.github.io@google-scholar-stats%2Fgs_data.json&query=$['publications']['NesqTz8AAAAJ:M_lZXyI38BkC']['num_citations']&labelColor=f6f6f6&color=9cf&style=flat&label=Citations)](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=NesqTz8AAAAJ&citation_for_view=NesqTz8AAAAJ:M_lZXyI38BkC) [![](https://img.shields.io/github/languages/code-size/lijfrank/GA2MIF?style=social)](https://github.com/lijfrank/GA2MIF)

- <a href="https://doi.org/10.1109/TAFFC.2024.3389453" class="no-underline">CFN-ESA: A cross-modal fusion network with emotion-shift awareness for dialogue emotion recognition</a>  
**Jiang Li**, Xiaoping Wang, Yingjian Liu, Zhigang Zeng  
IEEE Transactions on Affective Computing (``TAFFC``), 2024 [CAAI-A]  
[![](https://img.shields.io/badge/dynamic/json?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FLijfrank%2Flijfrank.github.io@google-scholar-stats%2Fgs_data.json&query=$['publications']['NesqTz8AAAAJ:idthP5jqfYAC']['num_citations']&labelColor=f6f6f6&color=9cf&style=flat&label=Citations)](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=NesqTz8AAAAJ&citation_for_view=NesqTz8AAAAJ:idthP5jqfYAC) [![](https://img.shields.io/github/languages/code-size/lijfrank/CFN-ESA?style=social)](https://github.com/lijfrank/CFN-ESA)

- <a href="https://doi.org/10.1109/TPAMI.2025.3581236" class="no-underline">Tracing intricate cues in dialogue: Joint graph structure and sentiment dynamics for multimodal emotion recognition</a>  
**Jiang Li**, Xiaoping Wang, Zhigang Zeng  
IEEE Transactions on Pattern Analysis and Machine Intelligence (``TPAMI``), 2025 [CAAI-A]  
[![](https://img.shields.io/badge/dynamic/json?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FLijfrank%2Flijfrank.github.io@google-scholar-stats%2Fgs_data.json&query=$['publications']['NesqTz8AAAAJ:WzTVkKNmPSkC']['num_citations']&labelColor=f6f6f6&color=9cf&style=flat&label=Citations)](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=NesqTz8AAAAJ&citation_for_view=NesqTz8AAAAJ:WzTVkKNmPSkC) [![](https://img.shields.io/github/languages/code-size/lijfrank/GraphSmile?style=social)](https://github.com/lijfrank/GraphSmile)

- <a href="https://doi.org/10.1007/s11432-023-3908-6" class="no-underline">EmotionIC: Emotional inertia and contagion-driven dependency modeling for emotion recognition in conversation</a>  
Yingjian Liuâ€ , **Jiang Li**â€ , Xiaoping Wang, Zhigang Zeng  
SCIENCE CHINA Information Sciences (``SCIS``), 2023 [CAAI-A]  
[![](https://img.shields.io/badge/dynamic/json?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FLijfrank%2Flijfrank.github.io@google-scholar-stats%2Fgs_data.json&query=$['publications']['NesqTz8AAAAJ:rbGdIwl2e6cC']['num_citations']&labelColor=f6f6f6&color=9cf&style=flat&label=Citations)](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=NesqTz8AAAAJ&citation_for_view=NesqTz8AAAAJ:rbGdIwl2e6cC) [![](https://img.shields.io/github/languages/code-size/lijfrank/EmotionIC?style=social)](https://github.com/lijfrank/EmotionIC)

- <a href="https://doi.org/10.1016/j.neucom.2023.126427" class="no-underline">GraphMFT: A graph network based multimodal fusion technique for emotion recognition in conversation</a>  
**Jiang Li**, Xiaoping Wang, Guoqing Lv, Zhigang Zeng  
Neurocomputing (``NEUCOM``), 2023 [CAAI-B]  
[![](https://img.shields.io/badge/dynamic/json?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FLijfrank%2Flijfrank.github.io@google-scholar-stats%2Fgs_data.json&query=$['publications']['NesqTz8AAAAJ:q0uBw5dMOAkC']['num_citations']&labelColor=f6f6f6&color=9cf&style=flat&label=Citations)](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=NesqTz8AAAAJ&citation_for_view=NesqTz8AAAAJ:q0uBw5dMOAkC) [![](https://img.shields.io/github/languages/code-size/lijfrank/GraphMFT?style=social)](https://github.com/lijfrank/GraphMFT)

- <a href="https://doi.org/10.1016/j.knosys.2024.111434" class="no-underline">ERNetCL: A novel emotion recognition network in textual conversation based on curriculum learning strategy</a>  
**Jiang Li**, Xiaoping Wang, Yingjian Liu, Zhigang Zeng  
Knowledge-Based Systems (``KBS``), 2024 [CAAI-B]  
[![](https://img.shields.io/badge/dynamic/json?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FLijfrank%2Flijfrank.github.io@google-scholar-stats%2Fgs_data.json&query=$['publications']['NesqTz8AAAAJ:g_UdREhPGEoC']['num_citations']&labelColor=f6f6f6&color=9cf&style=flat&label=Citations)](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=NesqTz8AAAAJ&citation_for_view=NesqTz8AAAAJ:g_UdREhPGEoC) [![](https://img.shields.io/github/languages/code-size/lijfrank/ERNetCL?style=social)](https://github.com/lijfrank/ERNetCL)

- <a href="https://doi.org/10.1016/j.engappai.2023.107530" class="no-underline">A dual-stream recurrence-attention network with globalâ€“local awareness for emotion recognition in textual dialog</a>  
**Jiang Li**, Xiaoping Wang, Zhigang Zeng  
Engineering Applications of Artificial Intelligence (``EAAI``), 2023 [CAAI-C]  
[![](https://img.shields.io/badge/dynamic/json?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FLijfrank%2Flijfrank.github.io@google-scholar-stats%2Fgs_data.json&query=$['publications']['NesqTz8AAAAJ:txeM2kYbVNMC']['num_citations']&labelColor=f6f6f6&color=9cf&style=flat&label=Citations)](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=NesqTz8AAAAJ&citation_for_view=NesqTz8AAAAJ:txeM2kYbVNMC) [![](https://img.shields.io/github/languages/code-size/lijfrank/DualRAN?style=social)](https://github.com/lijfrank/DualRAN)

- <a href="https://doi.org/10.1109/TCSS.2024.3477531" class="no-underline">Producing considerate responses: Progressive staged training for emotional support conversation</a>  
Guoqing Lvâ€ , **Jiang Li**â€ , Xiaoping Wang, Xin Zhan, Zhigang Zeng  
IEEE Transactions on Computational Social Systems (``TCSS``), 2024 [CAAI-C]  
[![](https://img.shields.io/badge/dynamic/json?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FLijfrank%2Flijfrank.github.io@google-scholar-stats%2Fgs_data.json&query=$['publications']['NesqTz8AAAAJ:X4-KO54GjGYC']['num_citations']&labelColor=f6f6f6&color=9cf&style=flat&label=Citations)](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=NesqTz8AAAAJ&citation_for_view=NesqTz8AAAAJ:X4-KO54GjGYC) [![](https://img.shields.io/github/languages/code-size/lijfrank/BlenderBot-ThTra?style=social)](https://github.com/lijfrank/BlenderBot-ThTra)

- <a href="https://doi.org/10.34133/cbsystems.0074" class="no-underline">A domain generalization and residual network-based emotion recognition from physiological signals</a>  
Junnan Liâ€ , **Jiang Li**â€ , Xiaoping Wang, Xin Zhan, Zhigang Zeng  
Cyborg and Bionic Systems (``CBS``), 2023  
[![](https://img.shields.io/badge/dynamic/json?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FLijfrank%2Flijfrank.github.io@google-scholar-stats%2Fgs_data.json&query=$['publications']['NesqTz8AAAAJ:BJtnxTr0fRcC']['num_citations']&labelColor=f6f6f6&color=9cf&style=flat&label=Citations)](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=NesqTz8AAAAJ&citation_for_view=NesqTz8AAAAJ:BJtnxTr0fRcC) <!-- [![](https://img.shields.io/github/languages/code-size/lijfrank/DGR-ERPS?style=social)](https://github.com/lijfrank/DGR-ERPS) -->

- <a href="" class="no-underline">A bio-inspired tactile-olfactory fusion perception system based on memristive spiking neural network</a>  
Chao Yang, Xiaoping Wang, Zhanfei Chen, **Jiang Li**, Nan Qin, Tingwen Huang, Zhigang Zeng  
SCIENCE CHINA Information Sciences (``SCIS``), 2025 [CAAI-A]

## Conference Paper

- <a href="https://escholarship.org/uc/item/0dk8x579" class="no-underline">InferEM: Inferring the speaker's intention for empathetic dialogue generation</a>  
Guoqing Lvâ€ , **Jiang Li**â€ , Xiaoping Wang, Zhigang Zeng  
The 45th Annual Meeting of the Cognitive Science Society (``CogSci 2023``), 26-29 July 2023, Sydney, Australia [CAAI-B]  
[![](https://img.shields.io/badge/dynamic/json?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FLijfrank%2Flijfrank.github.io@google-scholar-stats%2Fgs_data.json&query=$['publications']['NesqTz8AAAAJ:NtCmTCuxid4C']['num_citations']&labelColor=f6f6f6&color=9cf&style=flat&label=Citations)](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=NesqTz8AAAAJ&citation_for_view=NesqTz8AAAAJ:NtCmTCuxid4C) [![](https://img.shields.io/github/languages/code-size/lijfrank/InferEM?style=social)](https://github.com/lijfrank/InferEM)

- <a href="https://doi.org/10.1109/CAC59555.2023.10451247" class="no-underline">A spatio-temporal scale based affective neural network for emotion detection in conversation</a>  
**Jiang Li**, Xiaoping Wang, Shanglin Lei, Zhigang Zeng  
2023 China Automation Congress (``CAC 2023``), 17-19 November 2023, Chongqing, China  
[![](https://img.shields.io/badge/dynamic/json?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FLijfrank%2Flijfrank.github.io@google-scholar-stats%2Fgs_data.json&query=$['publications']['NesqTz8AAAAJ:Kqc1aDSOPooC']['num_citations']&labelColor=f6f6f6&color=9cf&style=flat&label=Citations)](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=NesqTz8AAAAJ&citation_for_view=NesqTz8AAAAJ:Kqc1aDSOPooC)

- <a href="" class="no-underline">HAFUNet: A hierarchical attention fusion network for monocular depth estimation integrating event and frame data</a>  
Siyuan Zhang, Xiaoping Wang, **Jiang Li**, Weibin Feng, Xin Zhan, Hongzhi Huang  
The 33rd ACM International Conference on Multimedia (``MM 2025``), 27-31 October 2025, Dublin, Ireland [CAAI-A]

- <a href="https://doi.org/10.1109/ICTAI59109.2023.00133" class="no-underline">Watch the speakers: A hybrid continuous attribution network for emotion recognition in conversation with emotion disentanglement</a>  
Shanglin Lei, Xiaoping Wang, Guanting Dong, **Jiang Li**, Zhigang Zeng  
The 35th IEEE International Conference on Tools with Artificial Intelligence (``ICTAI 2023``), 6-8 November 2023, Atlanta, USA [CAAI-C]  
[![](https://img.shields.io/badge/dynamic/json?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FLijfrank%2Flijfrank.github.io@google-scholar-stats%2Fgs_data.json&query=$['publications']['NesqTz8AAAAJ:GYcXSSpN504C']['num_citations']&labelColor=f6f6f6&color=9cf&style=flat&label=Citations)](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=NesqTz8AAAAJ&citation_for_view=NesqTz8AAAAJ:GYcXSSpN504C) <!-- [![](https://img.shields.io/github/languages/code-size/lijfrank/HCAN?style=social)](https://github.com/lijfrank/HCAN) -->

<!-- - <a class="no-underline">A novel text classification approach based on meta-path similarities and graph neural networks</a>  
H. Wang, **Jiang Li**, Q. Zhou, L. Ge  
The 33rd International Conference on Software Engineering and Knowledge Engineering (``SEKE 2021``), 1-10 July 2021, Pittsburgh, Pennsylvania, USA

- <a class="no-underline">Discovering the lonely among the students with weighted graph neural networks</a>  
Q. Zhou, **Jiang Li**, Y. Tang, H. Wang  
The 32nd IEEE International Conference on Tools with Artificial Intelligence (``ICTAI 2020``), 9â€“11 November 2020, Virtual Conference

- <a class="no-underline">Identifying loners from their project collaboration records - a graph-based approach</a>  
Q. Zhou, **Jiang Li**, Y. Tang, L. Ge  
The 13th International Conference on Knowledge Science, Engineering and Management (``KSEM 2020``), 28â€“30 August 2020, Hangzhou, China -->

## Other Paper
- <a href="" class="no-underline">Hypergraph missing modality learning for conversational affect detection</a>  
**Jiang Li**  
In Preparation, 2025

- <a href="" class="no-underline">Joint masked reconstruction and contrastive learning for mining interactions between proteins</a>  
**Jiang Li**, Siyuan Zhang, Weibin Feng, Xiaoping Wang  
In Submission, 2025

- <a href="" class="no-underline">Extracting inter-protein interactions via multitasking graph structure learning</a>  
**Jiang Li**, Weibin Feng, Siyuan Zhang, Xiaoping Wang  
In Submission, 2025

- <a href="https://arxiv.org/abs/2306.02098" class="no-underline">Large, complex, and realistic safety clothing and helmet detection: Dataset and method</a>  
Fusheng Yuâ€ , **Jiang Li**â€ , Xiaoping Wang, Depeng Li, Xin Zhan, Shaojin Wu, Junjie Zhang  
In Submission, 2025  
[![](https://img.shields.io/badge/dynamic/json?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FLijfrank%2Flijfrank.github.io@google-scholar-stats%2Fgs_data.json&query=$['publications']['NesqTz8AAAAJ:hHIA4WEVY-EC']['num_citations']&labelColor=f6f6f6&color=9cf&style=flat&label=Citations)](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=NesqTz8AAAAJ&citation_for_view=NesqTz8AAAAJ:hHIA4WEVY-EC) [![](https://img.shields.io/github/languages/code-size/lijfrank/SFCHD-SCALE?style=social)](https://github.com/lijfrank/SFCHD-SCALE)

- <a href="" class="no-underline">VeMamba: Voxel-based multi-scale state space model network for event stream recognition</a>  
Xin Zhan, Xiaoping Wang, Weibin Feng, Depeng Li, Hongzhi Huang, **Jiang Li**  
In Submission, 2025

- <a href="" class="no-underline">SNN-based lightweight denoising method for event cameras</a>  
Hongzhi Huang, Xiaoping Wang, Weibin Feng, Xin Zhan, **Jiang Li**  
In Submission, 2025

- <a href="" class="no-underline">ELGA-GMamba: An efficient local-to-global awareness graph mamba for object recognition with event-based cameras</a>  
Weibin Feng, Xiaoping Wang, Xin Zhan, Hongzhi Huang, Siyuan Zhang, **Jiang Li**  
In Submission, 2025

- <a href="" class="no-underline">SDAUT: Sparse-adaptive deformable attention U-Net Transformer for event-frame depth estimation</a>  
Siyuan Zhang, Xiaoping Wang, Weibin Feng, Honggang Yang, Hongzhi Huang, **Jiang Li**  
In Submission, 2025

- <a href="" class="no-underline">AGFI-Net: Adaptive graph feature interaction network for event-based action recognition</a>  
Weibin Feng, Xiaoping Wang, Siyuan Zhang, Hongzhi Huang, Xin Zhan, **Jiang Li**  
In Submission, 2025

<!-- ## Invention Patent
- A method, device, and computer for identifying psychological states of students (ZL202010406747.2)  
**Jiang Li**, Q. Zhou, C. Yin, J. Ou, H. Wang, X. Shi, W. Peng, D. Yang   
China National Intellectual Property Administration, 2023 -->